# 查询执行 2(Query Execution II)

---

## 概述

在数据库系统中有两种并行的查询处理技术。一个是单个查询中多个不同部分同时执行的`查询内并行` ，另一个是多个查询同时执行的`查询间并行`。查询间并行用于事务处理的数据库系统中提高系统的事务吞吐量。查询内并行用于加速执行时间长的查询。每一个操作被并行的执行，也就是并行执行每个操作内的子操作，被称为`操作内并行`。多个操作并行执行称为`操作间并行`。

---

## 处理模型

**每个DBMS工作者一个进程** 依赖操作系统的调度。工作者通过共享内存来访问全局数据。单工作者出现故障不会对其他工作者造成影响。

**每个DBMS工作者一个线程** DBMS使用它自己的调度器来调度线程。可能有一个分配者线程。一个线程出现故障可能会造成整个系统崩溃。在此模型中，需要考虑的调度问题包括：一个工作者多少个任务？它的结果输出到哪里？分配多少个CPU核给一个工作者？任务在哪个核上执行？

**内嵌DBMS** DMBS与应用程序运行在一个内存地址空间中。

---

## 操作内并行

### 并行排序

并行排序有两种方法。一种是`范围划分排序`，另一种是`并行的外部排序归并`。

**范围划分排序** 先将数据按照范围划分给N给节点，每个节点对自己的数据集进行排序。随后将每个节点的结果合并起来。这种方法可能造成分配不均的情况，解决办法是先将原数据集按更细的范围划分给M个虚拟节点(一般而言，M是N的若干倍数)，然后按照round-robin的方式将虚拟节点映射为物理节点。

**并行的外部归并排序** 先将原数据集无限制地划分给N个节点，然后节点做本地排序。随后每个节点按照范围划分要求把数据集通过排序流发给相应的物理节点做合并，最后拼接所有的物理节点合并结果。这种方法可能造成执行不均的情况，解决办法是每个节点交叉地发送每个数据序列的块。

### 并行连接

并行连接先将原数据集划分为若干个分块，然后将每个节点将本地数据集按照划分原则将数据发送到不同的对应分块，最后将若干个节点的本地连接结果合并起来得到最终结果。划分方法有按范围划分和用哈希函数划分两种。在节点本地的连接有多种算法。

#### 基于划分的连接

**基于划分的并行哈希连接** 当数据到达节点时，把数据写入对应的分块中。等待分块建立完成后，执行build-probe阶段得到最终结果。当其中一个关系的分块小到相当大的一部分能够在每个节点中都能放在内存中时，使用杂合哈希连接，此时，更大的关系的元组到来时，划分后直接比较像等条件而非写入磁盘。

**基于划分的并行归并连接** 当元组到来时，直接排序后生成run而非直接写入磁盘。

**基于划分的并行嵌套循环连接** 和 **基于划分的并行结合索引的嵌套循环连接** 是在本地上结合使用嵌套循环连接和结合索引的嵌套循环连接两种技术。

#### 分片-复制连接

当涉及到带有不等条件的连接时，基于划分的并行连接不再适用。有两种方法实现分片-复制连接。

**非对称的分片-复制连接** 将关系r划分为N个片，每个片在一个节点上。然后每个节点都有一个完整的s关系。最后，每个节点在本地上计算连接结果。

**对称的分片-复制连接** 假定有N * M个节点，将关系r,s分为N,M个片，然后节点N(i,j) 上有r(i)和s(j)分片。接下来在本地上计算连接结果，最后合并。

#### 处理并行连接中的偏斜问题

一般来说，分配均匀、随机性高的哈希函数能够处理好负载问题。相比之下，范围划分比较脆弱，除非范围选择相当仔细。处理偏斜问题有两种办法。

**工作前避免偏斜** 基于round-robin窃取分发的虚拟节点划分能够在物理节点工作前尽可能减少偏斜。

**工作中避免偏斜** 在工作中如果发现有节点工作负载在一定程度上大于其他节点，那么系统就分配一些该物理节点上的虚拟节点给空闲的物理节点工作。

### 其它操作

#### 其他关系操作

**选择** 划分后在每个节点内对条件进行判断即可。

**去重** 一种方法是划分后在节点内对数据集进行排序，最后合并去重。另一种方法是直接在节点内去重。

**投影** 每个节点并行地读取某一列的某部分即可。

**聚合** 划分后在节点内计算结果即可。优化的办法是在划分前先在本地进行一次*部分聚合* 然后划分后进行再次聚合得到最终结果。

#### 映射和规约操作

把原数据集分给M个映射工作者完成，得到若干个中间键值对，然后中间键值对通过哈希函数或者范围划分分配个N个规约工作者完成。待所有任务完成后，合并规约工作者的计算结果得到整个系统的结果。

---

## 查询计划的并行求值

### 操作间并行

**基于流水线的并行** 生产者在产生结果的同时，消费者也在消费结果。其优点是能够避免中间结果写入磁盘而产生的开销。缺点是1）一般来说，随着长度的增大，并行度不会有很大的提高；2）有些操作符并不满足使用流水线的条件；3）当有部分操作的开销远大于其它操作的开销时，只能获得很小的加速效果。在实现上，选择推模型来实现流水线，因为这能够带来更高的并行性，虽然会有互斥锁的开销或者网络传输的开销。

**独立的并行** 一个查询表达式中不依赖于其他操作操作的操作可以并行执行。同基于流水线的并行一样，只能提供小程度的并行性。

### 交换运算符模型

交换运算符通过一种特定的方式来重划分数据。所有的数据交换都由交换运算符来完成。交换运算符有两个组成成分：1）在源节点的划分策略；2）在目的节点的归并策略。也就是说，在源节点处封装了划分和发送这两个操作以及在目的节点处封装了接受和归并这两个操作。其好处是能够让原有的数据库系统代码无需做大量的更改就能够被并行化。但有的时候也需要能够让运算符知道并行的内部信息，比如嵌套循环连接的索引表在另一节点中而当前节点需要使用那个索引。

交换运算符有三种类型：聚集(Gather), 分发(Distribute) 和 重划分(Repartition)

**聚集** 将并行执行操作的输出结果合并为一个结果。例如，在处理操作内并行的选择时，每个子操作对其局部的数据进行选择，然后把结果输出聚集操作运算符。

**分发** 将大量的数据划分后发送给不同的节点执行。例如，在处理二元连接操作中，把两个关系按照一定规则划分后发送给不同的节点执行，从而使得操作并行化。

**重划分** 将本已划分的数据按照新的规则划分后发送给不同节点。例如，某一查询要求的优化划分方式与已经存储好的数据的划分方式不同，为提高执行效率，需要重新划分。

### 查询计划的容灾

在映射规约模型中，先将任务分配个映射工作者，等待所有映射完成后，调度者为规约工作者分配任务，规约工作者把结果写入分布式文件系统中。容灾保障的处理方法是：通过后援任务来重新执行故障节点的任务。映射规约模型的等待环节使得其不能够流水线化。

Apache Spark使用Resilience Distributed Datasets来实现容灾保障。其思想是每个操作的输入和输出都是一个RDD，为每个RDD记录施加于其的操作。当这个涉及这个RDD的通信出现故障后，重新执行所记录的操作。但这个方法比较耗时，优化方法是在一个节点准备将结果输出给下一个节点时，把待发送结果保存到本地，如果出现故障就重发结果即可。

允许流水线处理数据同时实现容灾保障的一个想法是在节点接收数据时，记录其接收到了哪些数据。一旦传输出现故障，就重启源节点。这种做法类似于TCP包传输中的序列号方法保证流水线传输。

---

## 共享内存架构中的查询处理

在大规模系统架构中，外层是非共享架构，而内层是内存共享架构。一般在共享内存架构中的查询处理使用的是多线程而非多进程。使用共享内存架构的优化案例包括非对称分片-复制连接的复制部分最小化，以及解决负载均衡问题时的工作窃取变得更容易。

在现代处理器中，存在非同一内存访问(Non-Uniform Memory Access)的情况，需要注意访问数据的同步性。同时也考虑CPU缓存的效果。SIMD现在已经用于处理数据库系统中的并行操作。

---

## 为并行执行的查询优化

并行执行的优化比顺序执行的优化更难，原因有二：1）并行执行的优化选择空间更多；2）并行执行的开销模型更复杂。

### 并行查询计划空间

同顺序执行一样，并行执行的优化需要考虑操作符树、操作符的注解指令、流水线以及中间结果物化。不一样的是，并行查询的优化需要额外考虑怎么并行以及怎么调度。

### 并行查询求值的开销

有两种模型来表示查询的开销。一个是*资源消耗代价模型* ，另一个是*响应时间代价模型*。 

**资源消耗代价模型** 需要累加CPU运算的代价、I/O执行的代价以及通信传输的代价。

**响应时间代价模型** 在并行的操作中的最大代价。另外，操作的启动代价和倾斜代价也需要考虑。

### 选择一个并行查询计划

完全考虑所有的并行查询计划空间会产生不小的开销。所以，我们一般采用启发式方式来优化。

**方法一** 选择最好的顺序执行计划以及最优的并行化方法。在第一阶段，不考虑如何并行化和如何调度，仅仅选择最好的顺序执行计划。在第二阶段，考虑并行化方式和调度方式。

**方法二** 假定每个操作都在所有节点上并行执行，选择最好的查询计划。考虑了怎么划分。

**方法三** 在物理存储层上优化，在同一数据集的不同备份上按照不同的属性来划分。

### 数据的并置

如果查询需要的所有数据(数据量不会太大)都在同一节点，那么查询可以被很快的返回。为了实现这一目标，一个优化技术就是将一个查询所需要的数据都放在同一个节点。如果所需要的不同关系按照不同的划分方式划分，数据并置可能不能直接工作。为尽可能避免这一情况，一个简单的处理办法就是同一数据集在不同处的备份按照不同的方式划分来存储。

### 物化视图的并行维护

在并行数据库中，物化视图可能会有相当大量的数据，从而不得不把数据存放在多个节点。物化视图加快了查询的响应速度，同时也带来了视图维护处理更新的时间开销。在简单的视图中，只需要一次划分与本地更改就能更新物化视图。然而，在复杂的视图中，需要每个节点都要维护第一次计算视图的划分输入与输出。当视图更新时，按照最初的划分方式，让一个节点更新其输入与输出，如果该节点输出是结果输出，那么就结束，否则把再划分输出让下一个节点更新。

---

## 流式数据的并行处理

并行流处理系统需要有大量的数据入口点。输入数据被路由到流处理系统中。

### 元组的路由

元组的路由分为*逻辑路由* 和 *物理路由* 两种。

**逻辑路由** 一种实现方法是在流式数据处理系统通过读配置文件来生成逻辑路由直向无环图。另一种实现是发布-订阅系统。发布者向相关的主题发布内容，也就是把数据发给每一个订阅者，而订阅者通过订阅某一主题来获得内容。

**物理路由** 元组到达某一运算符节点，运算符节点将到达元组路由到某一物理节点。

### 流式操作的并行处理

对于标准的关系运算操作，已有的处理并行运算的技术也可以用于处理流式数据。在允许用户自定义操作的系统中，每个元组需要有一个关联键，以便于让具有不同键的元组发送给不同的节点从而实现并行处理。流式操作通常需要存储状态。

### 流式数据的容灾

重启并执行的容灾处理手段在流式数据处理系统中不合适。原因包括延迟较高以及出现重复结果。

在流式数据处理系统中处理容灾的手段包括：

* Kafka中每个Topic-partition存放在多个节点上并且是存放在磁盘中。

* 每个运算符节点周期性地记录检查点并存储在分布式文件系统中，一旦出现单点故障，那么系统立刻拉起另一个节点从最后一个检查点开始执行。

* 每个运算符节点有一个相同的复制节点，从而保证即使有一个节点出现故障也能够提供低延迟保证。

---

## I/O 并行

当查询计划的主要瓶颈是I/O时，CPU的并行执行不能优化查询计划的整体性能。另外，如果操作需要在多个磁盘读取数据时，也会降低查询计划的性能。

**多磁盘并行** 数据库文件存储在多个磁盘设备上。

**数据库划分** 数据库文件被划分为多个不相交的子集并独立存储与管理。  
